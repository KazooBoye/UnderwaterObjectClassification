"""
Approach 1: YOLOv8-based One-Stage Detector for Underwater Object Detection
GPU-accelerated TensorFlow implementation with underwater-specific optimizations
"""

import os
import sys
import numpy as np
import tensorflow as tf
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import datetime

# Optional wandb import
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    print("Warning: wandb not available. Experiment tracking disabled.")
    WANDB_AVAILABLE = False

# Add utils to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utils'))
from common_utils import UnderwaterDatasetConfig, GPUSetup, DataLoader, MetricsCalculator, Visualizer

class CSPDarknet(tf.keras.Model):
    """CSPDarknet backbone for YOLOv8"""
    
    def __init__(self, depth_multiple=0.67, width_multiple=0.75):
        super(CSPDarknet, self).__init__()
        self.depth_multiple = depth_multiple
        self.width_multiple = width_multiple
        
        # Define channel sizes
        channels = [64, 128, 256, 512, 1024]
        channels = [int(c * width_multiple) for c in channels]
        
        # Stem
        self.stem = tf.keras.Sequential([
            tf.keras.layers.Conv2D(channels[0], 3, strides=2, padding='same', use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('swish')
        ])
        
        # CSP Blocks
        self.layer1 = self._make_csp_layer(channels[0], channels[1], num_blocks=1, stride=2)
        self.layer2 = self._make_csp_layer(channels[1], channels[2], num_blocks=2, stride=2)
        self.layer3 = self._make_csp_layer(channels[2], channels[3], num_blocks=2, stride=2)
        self.layer4 = self._make_csp_layer(channels[3], channels[4], num_blocks=1, stride=2)
        
    def _make_csp_layer(self, in_channels, out_channels, num_blocks, stride):
        """Create CSP layer"""
        layers = []
        
        # Downsample
        layers.append(tf.keras.Sequential([
            tf.keras.layers.Conv2D(out_channels, 3, strides=stride, padding='same', use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('swish')
        ]))
        
        # CSP Bottleneck
        for _ in range(int(num_blocks * self.depth_multiple)):
            layers.append(CSPBottleneck(out_channels, out_channels))
            
        return tf.keras.Sequential(layers)
    
    def call(self, x, training=False):
        x = self.stem(x, training=training)
        x1 = self.layer1(x, training=training)  # P1
        x2 = self.layer2(x1, training=training)  # P2
        x3 = self.layer3(x2, training=training)  # P3
        x4 = self.layer4(x3, training=training)  # P4
        
        return x2, x3, x4  # Return P3, P4, P5 for FPN

class CSPBottleneck(tf.keras.layers.Layer):
    """CSP Bottleneck layer"""
    
    def __init__(self, in_channels, out_channels, shortcut=True):
        super(CSPBottleneck, self).__init__()
        self.shortcut = shortcut and in_channels == out_channels
        
        hidden_channels = out_channels // 2
        
        self.conv1 = tf.keras.Sequential([
            tf.keras.layers.Conv2D(hidden_channels, 1, use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('swish')
        ])
        
        self.conv2 = tf.keras.Sequential([
            tf.keras.layers.Conv2D(out_channels, 3, padding='same', use_bias=False),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('swish')
        ])
        
    def call(self, x, training=False):
        identity = x
        
        x = self.conv1(x, training=training)
        x = self.conv2(x, training=training)
        
        if self.shortcut:
            x = x + identity
            
        return x

class PANFPN(tf.keras.layers.Layer):
    """Path Aggregation Network with Feature Pyramid Network"""
    
    def __init__(self, channels=256):
        super(PANFPN, self).__init__()
        self.channels = channels
        
        # Lateral connections
        self.lateral_conv3 = tf.keras.layers.Conv2D(channels, 1, use_bias=False)
        self.lateral_conv4 = tf.keras.layers.Conv2D(channels, 1, use_bias=False)
        self.lateral_conv5 = tf.keras.layers.Conv2D(channels, 1, use_bias=False)
        
        # Top-down path
        self.td_conv4 = tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)
        self.td_conv3 = tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)
        
        # Bottom-up path
        self.bu_conv3 = tf.keras.layers.Conv2D(channels, 3, strides=2, padding='same', use_bias=False)
        self.bu_conv4 = tf.keras.layers.Conv2D(channels, 3, strides=2, padding='same', use_bias=False)
        
        # Final convolutions
        self.out_conv3 = tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)
        self.out_conv4 = tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)
        self.out_conv5 = tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)
        
    def call(self, features, training=False):
        """
        features: [P3, P4, P5] from backbone
        returns: [P3_out, P4_out, P5_out] enhanced features
        """
        p3, p4, p5 = features
        
        # Lateral connections
        lat_p3 = self.lateral_conv3(p3, training=training)
        lat_p4 = self.lateral_conv4(p4, training=training)
        lat_p5 = self.lateral_conv5(p5, training=training)
        
        # Top-down path
        td_p4 = lat_p4 + tf.image.resize(lat_p5, tf.shape(lat_p4)[1:3])
        td_p4 = self.td_conv4(td_p4, training=training)
        
        td_p3 = lat_p3 + tf.image.resize(td_p4, tf.shape(lat_p3)[1:3])
        td_p3 = self.td_conv3(td_p3, training=training)
        
        # Bottom-up path
        bu_p4 = td_p4 + self.bu_conv3(td_p3, training=training)
        bu_p5 = lat_p5 + self.bu_conv4(bu_p4, training=training)
        
        # Final outputs
        out_p3 = self.out_conv3(td_p3, training=training)
        out_p4 = self.out_conv4(bu_p4, training=training)
        out_p5 = self.out_conv5(bu_p5, training=training)
        
        return [out_p3, out_p4, out_p5]

class YOLOv8Head(tf.keras.layers.Layer):
    """YOLOv8 Detection Head"""
    
    def __init__(self, num_classes=7, num_anchors=1, channels=256):
        super(YOLOv8Head, self).__init__()
        self.num_classes = num_classes
        self.num_anchors = num_anchors
        
        # Classification head
        self.cls_convs = []
        self.reg_convs = []
        
        for i in range(2):  # 2 conv layers
            self.cls_convs.append(tf.keras.Sequential([
                tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('swish')
            ]))
            
            self.reg_convs.append(tf.keras.Sequential([
                tf.keras.layers.Conv2D(channels, 3, padding='same', use_bias=False),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('swish')
            ]))
        
        # Final prediction layers
        self.cls_pred = tf.keras.layers.Conv2D(
            num_classes * num_anchors, 3, padding='same',
            bias_initializer=tf.constant_initializer(-np.log((1 - 0.01) / 0.01))
        )
        
        self.reg_pred = tf.keras.layers.Conv2D(4 * num_anchors, 3, padding='same')
        self.obj_pred = tf.keras.layers.Conv2D(1 * num_anchors, 3, padding='same')
        
    def call(self, features, training=False):
        """
        features: List of feature maps from FPN [P3, P4, P5]
        returns: List of predictions for each level
        """
        predictions = []
        
        for feat in features:
            # Classification branch
            cls_feat = feat
            for conv in self.cls_convs:
                cls_feat = conv(cls_feat, training=training)
            cls_pred = self.cls_pred(cls_feat)
            
            # Regression branch
            reg_feat = feat
            for conv in self.reg_convs:
                reg_feat = conv(reg_feat, training=training)
            reg_pred = self.reg_pred(reg_feat)
            obj_pred = self.obj_pred(reg_feat)
            
            # Reshape predictions
            batch_size = tf.shape(feat)[0]
            height, width = tf.shape(feat)[1], tf.shape(feat)[2]
            
            cls_pred = tf.reshape(cls_pred, [batch_size, height, width, self.num_anchors, self.num_classes])
            reg_pred = tf.reshape(reg_pred, [batch_size, height, width, self.num_anchors, 4])
            obj_pred = tf.reshape(obj_pred, [batch_size, height, width, self.num_anchors, 1])
            
            # Combine predictions
            pred = tf.concat([reg_pred, obj_pred, cls_pred], axis=-1)
            predictions.append(pred)
        
        return predictions

class UnderwaterYOLOv8(tf.keras.Model):
    """Complete YOLOv8 model for underwater object detection"""
    
    def __init__(self, num_classes=7, depth_multiple=0.67, width_multiple=0.75):
        super(UnderwaterYOLOv8, self).__init__()
        
        self.num_classes = num_classes
        self.backbone = CSPDarknet(depth_multiple, width_multiple)
        self.neck = PANFPN(channels=int(256 * width_multiple))
        self.head = YOLOv8Head(num_classes, num_anchors=1, channels=int(256 * width_multiple))
        
        # Anchors for each scale (generated automatically or predefined)
        self.anchors = self._generate_anchors()
        
    def _generate_anchors(self):
        """Generate anchor boxes for each scale"""
        # These are approximate anchors - in practice, they should be optimized for the dataset
        anchors = [
            # P3 (8x downsampling) - for small objects
            [[10, 13], [16, 30], [33, 23]],
            # P4 (16x downsampling) - for medium objects  
            [[30, 61], [62, 45], [59, 119]],
            # P5 (32x downsampling) - for large objects
            [[116, 90], [156, 198], [373, 326]]
        ]
        return anchors
    
    def call(self, x, training=False):
        # Feature extraction
        features = self.backbone(x, training=training)
        
        # Feature fusion
        enhanced_features = self.neck(features, training=training)
        
        # Detection
        predictions = self.head(enhanced_features, training=training)
        
        return predictions

class YOLOv8Loss(tf.keras.losses.Loss):
    """YOLOv8 Loss Function with class weighting for imbalanced dataset"""
    
    def __init__(self, num_classes=7, class_weights=None, **kwargs):
        super(YOLOv8Loss, self).__init__(**kwargs)
        self.num_classes = num_classes
        self.class_weights = class_weights or [1.0] * num_classes
        
    def call(self, y_true, y_pred):
        """
        y_true: Ground truth [batch_size, num_objects, 6] (x, y, w, h, class, objectness)
        y_pred: Predictions from model
        """
        # This is a simplified version - full YOLO loss is complex
        # For production, consider using existing YOLO implementations
        
        total_loss = 0.0
        
        for i, pred in enumerate(y_pred):
            # Extract predictions
            batch_size, height, width, anchors, features = pred.shape
            
            # Reshape for processing
            pred = tf.reshape(pred, [batch_size, height * width * anchors, features])
            
            # Split predictions
            pred_bbox = pred[..., :4]  # x, y, w, h
            pred_obj = pred[..., 4:5]  # objectness
            pred_cls = pred[..., 5:]   # class probabilities
            
            # Compute loss components (simplified)
            bbox_loss = tf.reduce_mean(tf.square(pred_bbox - 0))  # Placeholder
            obj_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
                labels=tf.zeros_like(pred_obj), logits=pred_obj
            ))
            cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=tf.zeros([batch_size, height * width * anchors], dtype=tf.int32),
                logits=pred_cls
            ))
            
            # Combine losses with weights
            level_loss = 5.0 * bbox_loss + 1.0 * obj_loss + 0.5 * cls_loss
            total_loss += level_loss
        
        return total_loss

class YOLOv8Trainer:
    """Training pipeline for underwater YOLOv8"""
    
    def __init__(self, config_path_or_dict):
        # Handle both config file path and config dictionary
        if isinstance(config_path_or_dict, str):
            self.config = self._load_config(config_path_or_dict)
        elif isinstance(config_path_or_dict, dict):
            self.config = config_path_or_dict
        else:
            raise ValueError("Config must be either a file path (str) or dictionary")
            
        self.dataset_config = UnderwaterDatasetConfig()
        
        # Setup GPU
        self.gpu_setup = GPUSetup()
        self.gpu_available = self.gpu_setup.configure_gpu()
        self.strategy = self.gpu_setup.get_strategy()
        
        # Initialize components
        self.data_loader = DataLoader(self.dataset_config)
        self.metrics_calculator = MetricsCalculator(self.dataset_config)
        self.visualizer = Visualizer(self.dataset_config)
        
        # Initialize model and training components
        self.model = None
        self.optimizer = None
        self.loss_fn = None
        
        # Experiment tracking
        self.setup_experiment_tracking()
        
    def _load_config(self, config_path: str) -> Dict:
        """Load training configuration"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config
    
    def setup_experiment_tracking(self):
        """Setup Weights & Biases experiment tracking"""
        if WANDB_AVAILABLE and 'experiment' in self.config:
            wandb.init(
                project=self.config.get('experiment', {}).get('project', 'underwater-detection'),
                name=self.config.get('experiment', {}).get('name', 'yolov8-experiment'),
                tags=self.config.get('experiment', {}).get('tags', ['yolov8']),
                config=self.config
            )
        else:
            print("Experiment tracking disabled (wandb not available or no experiment config)")
    
    def build_model(self):
        """Build YOLOv8 model with distribution strategy"""
        with self.strategy.scope():
            # Model configuration - handle both nested and flattened config
            if 'model' in self.config:
                model_config = self.config['model']
                variant = model_config.get('variant', 'yolov8m')
            else:
                # Handle flattened config
                variant = self.config.get('variant', 'yolov8m')
            
            depth_multiple = 0.67 if variant == 'yolov8m' else 0.5
            width_multiple = 0.75 if variant == 'yolov8m' else 0.5
            
            # Create model
            self.model = UnderwaterYOLOv8(
                num_classes=self.dataset_config.num_classes,
                depth_multiple=depth_multiple,
                width_multiple=width_multiple
            )
            
            # Build model with dummy input
            dummy_input = tf.random.normal([1, *self.dataset_config.image_size, 3])
            _ = self.model(dummy_input)
            
            # Setup optimizer - handle both nested and flattened config
            if 'training' in self.config:
                learning_rate = float(self.config['training']['learning_rate'])
                weight_decay = float(self.config['training']['weight_decay'])
                class_weights = self.config['training']['class_weights']
            else:
                # Handle flattened config
                learning_rate = float(self.config.get('learning_rate', 1e-3))
                weight_decay = float(self.config.get('weight_decay', 5e-4))
                class_weights = self.config.get('class_weights', None)
                
            self.optimizer = tf.keras.optimizers.AdamW(
                learning_rate=learning_rate,
                weight_decay=weight_decay
            )
            
            # Setup loss function
            self.loss_fn = YOLOv8Loss(
                num_classes=self.dataset_config.num_classes,
                class_weights=class_weights
            )
            
            # Compile model
            self.model.compile(
                optimizer=self.optimizer,
                loss=self.loss_fn,
                run_eagerly=False  # Enable graph mode for better performance
            )
        
        print(f"Model built successfully on {self.strategy.num_replicas_in_sync} device(s)")
        self.model.summary()
    
    def prepare_datasets(self):
        """Prepare training and validation datasets"""
        print("Preparing datasets...")
        
        # Handle both nested and flattened config
        if 'training' in self.config:
            batch_size = self.config['training']['batch_size']
        else:
            batch_size = self.config.get('batch_size', 16)
        
        # Create datasets
        self.train_dataset = self.data_loader.create_tf_dataset('train', batch_size)
        self.val_dataset = self.data_loader.create_tf_dataset('val', batch_size)
        
        # Distribute datasets
        self.train_dataset = self.strategy.experimental_distribute_dataset(self.train_dataset)
        self.val_dataset = self.strategy.experimental_distribute_dataset(self.val_dataset)
        
        print("Datasets prepared successfully")
    
    def train(self, train_dataset=None, val_dataset=None, epochs=None, callbacks=None):
        """Main training loop"""
        print("Starting training...")
        
        # Build model if not already built
        if self.model is None:
            print("Building model...")
            self.build_model()
        
        # Prepare datasets if not already prepared
        if not hasattr(self, 'train_dataset') or self.train_dataset is None:
            print("Preparing datasets...")
            self.prepare_datasets()
        
        # Use provided parameters or defaults
        if epochs is None:
            epochs = self.config.get('epochs', 150)
        if train_dataset is None:
            train_dataset = self.train_dataset
        if val_dataset is None:
            val_dataset = self.val_dataset
        if callbacks is None:
            callbacks = self._setup_callbacks()
        
        # Training loop
        with self.strategy.scope():
            history = self.model.fit(
                train_dataset,
                epochs=epochs,
                validation_data=val_dataset,
                callbacks=callbacks,
                verbose=1
            )
        
        # Plot training curves
        self.visualizer.plot_training_curves(
            history.history,
            save_path=f"training_curves_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        )
        
        return history
    
    def _setup_callbacks(self):
        """Setup training callbacks"""
        callbacks = []
        
        # Model checkpointing
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath='best_yolov8_model.h5',
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False,
            mode='min',
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # Early stopping
        if self.config['early_stopping']['enabled']:
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor=self.config['early_stopping']['monitor'],
                patience=self.config['early_stopping']['patience'],
                mode=self.config['early_stopping']['mode'],
                min_delta=self.config['early_stopping']['min_delta'],
                verbose=1,
                restore_best_weights=True
            )
            callbacks.append(early_stopping)
        
        # Learning rate scheduling
        lr_scheduler = tf.keras.callbacks.CosineRestartScheduler(
            first_restart_step=50,
            t_mul=2.0,
            m_mul=0.8,
            alpha=0.01
        )
        callbacks.append(lr_scheduler)
        
        # Weights & Biases callback (if available)
        if WANDB_AVAILABLE:
            wandb_callback = wandb.keras.WandbCallback(
                monitor='val_loss',
                mode='min',
                save_model=False
            )
            callbacks.append(wandb_callback)
        
        return callbacks
    
    def evaluate(self, test_dataset=None):
        """Evaluate model performance"""
        print("Evaluating model...")
        
        if self.model is None:
            print("Model not initialized. Please build model first.")
            return None
        
        # Use provided test dataset or load default
        if test_dataset is None:
            test_dataset = self.data_loader.create_tf_dataset('test', batch_size=1)
        
        # Run basic evaluation
        test_loss = self.model.evaluate(test_dataset, verbose=1)
        print(f"Test Loss: {test_loss}")
        
        # Get predictions for detailed metrics
        predictions = []
        ground_truths = []
        
        print("Computing predictions for mAP calculation...")
        for batch in test_dataset.take(100):  # Limit to avoid memory issues
            images, targets = batch
            try:
                pred = self.model(images, training=False)
                
                # Convert predictions to standard format
                for i in range(len(images)):
                    # Process YOLOv8 output - assuming it returns detection boxes
                    if hasattr(pred, 'numpy'):
                        pred_data = pred[i].numpy() if len(pred.shape) > 2 else pred.numpy()
                    else:
                        pred_data = []
                    
                    gt_boxes = targets['boxes'][i].numpy()
                    gt_labels = targets['labels'][i].numpy()
                    
                    # Placeholder processing - needs to be adapted based on actual model output
                    if isinstance(pred_data, np.ndarray) and len(pred_data.shape) > 1:
                        predictions.append({
                            'boxes': pred_data[:, :4] if pred_data.shape[1] >= 4 else [],
                            'scores': pred_data[:, 4] if pred_data.shape[1] > 4 else np.ones(len(pred_data)),
                            'labels': pred_data[:, 5] if pred_data.shape[1] > 5 else np.zeros(len(pred_data))
                        })
                    else:
                        predictions.append({
                            'boxes': [],
                            'scores': [],
                            'labels': []
                        })
                    
                    ground_truths.append({
                        'boxes': gt_boxes,
                        'labels': gt_labels
                    })
            except Exception as e:
                print(f"Error processing batch: {e}")
                continue
        
        # Calculate mAP using metrics calculator
        try:
            map_score = self.metrics_calculator.calculate_map(predictions, ground_truths)
            print(f"mAP Score: {map_score}")
        except Exception as e:
            print(f"Error calculating mAP: {e}")
            map_score = 0.0
        
        return {
            'test_loss': test_loss,
            'map_score': map_score
        }

def main():
    """Main training script"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train YOLOv8 for underwater object detection')
    parser.add_argument('--config', type=str, 
                       default='../configs/yolov8_config.yaml',
                       help='Path to configuration file')
    args = parser.parse_args()
    
    # Initialize trainer
    trainer = YOLOv8Trainer(args.config)
    
    # Build model
    trainer.build_model()
    
    # Prepare datasets
    trainer.prepare_datasets()
    
    # Train model
    history = trainer.train()
    
    # Evaluate model
    test_results = trainer.evaluate()
    
    # Save final results
    results = {
        'training_history': history.history,
        'test_results': test_results,
        'config': trainer.config
    }
    
    # Save results
    import json
    with open(f'yolov8_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print("Training completed successfully!")

if __name__ == "__main__":
    main()
